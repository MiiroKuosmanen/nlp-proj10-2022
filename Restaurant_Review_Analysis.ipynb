{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restaurant Reviews - Sentiment Analysis and Machine Learning\n",
    "\n",
    "*Ville KylmÃ¤maa, Joona Holappa, Miiro Kuosmanen, Anssi Valjakka*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T18:45:09.740478Z",
     "start_time": "2022-04-15T18:45:07.101021Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anssi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/anssi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/anssi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/anssi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/anssi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Python built-in modules\n",
    "import sys\n",
    "import os.path\n",
    "import json\n",
    "import subprocess\n",
    "import shlex\n",
    "from collections import Counter\n",
    "\n",
    "# %pip install pandas\n",
    "import pandas as pd\n",
    "\n",
    "# %pip install numpy\n",
    "import numpy as np\n",
    "\n",
    "# %pip install nltk\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# %pip install scipy\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# %pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %pip install scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# %pip install empath\n",
    "from empath import Empath\n",
    "\n",
    "# %pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "from nltk.corpus import genesis\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.snowball import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T18:45:18.492995Z",
     "start_time": "2022-04-15T18:45:09.743821Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read restaurant reviews to pandas dataframe format\n",
    "reviews_df = pd.read_csv('Restaurant_Reviews.tsv', sep='\\t')\n",
    "print(reviews_df)\n",
    "\n",
    "# Separate lists for Review and Liked columns\n",
    "review_column = reviews_df[\"Review\"]\n",
    "liked_column = reviews_df[\"Liked\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\nThe number of dislike (0) reviews in the data:\\n{liked_column.tolist().count(0)}\")\n",
    "print(f\"\\nThe number of like (1) reviews in the data:\\n{liked_column.tolist().count(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 1\n",
    "\n",
    "Use initially SentiStrength SentiStrength - sentiment strength detection in short texts - sentiment analysis,\n",
    "opinion mining (http://sentistrength.wlv.ac.uk/) implementation of sentiment, which provides negative and positive\n",
    "sentiment score, compute Pearson correlation between this constructed sentiment polarity and the annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize SentiStrength\n",
    "# http://sentistrength.wlv.ac.uk/\n",
    "senti_strength_path = \"./sentistrength/SentiStrength.jar\"\n",
    "language_folder_path = \"./sentistrength/SentiStrength_Data/\"\n",
    "\n",
    "# Check that the files exist in the given paths\n",
    "if not os.path.isfile(senti_strength_path):\n",
    "    print(\"SentiStrength not found at: \", senti_strength_path)\n",
    "if not os.path.isdir(language_folder_path):\n",
    "    print(\"SentiStrength data folder not found at: \", language_folder_path)\n",
    "\n",
    "# Returns SentiStrength sentiment score for the given string\n",
    "def rate_sentiment(sentiString):\n",
    "    # Open a subprocess using shlex to get the command line string into the correct args list format.\n",
    "    p = subprocess.Popen(shlex.split(\"java -jar '\" + senti_strength_path + \"' stdin sentidata '\" + language_folder_path + \"'\"),\n",
    "                         stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    # Communicate via stdin the string to be rated. Note that all spaces are replaced with +.\n",
    "    # Can't send string in Python 3, must send bytes.\n",
    "    b = bytes(sentiString.replace(\" \",\"+\"), 'utf-8')\n",
    "    stdout_byte, stderr_text = p.communicate(b)\n",
    "    stdout_text = stdout_byte.decode(\"utf-8\")\n",
    "    # Remove the tab spacing between the positive and negative ratings. e.g. \"1    -5\" -> \"1 -5\"\n",
    "    stdout_text = stdout_text.rstrip().replace(\"\\t\",\" \")\n",
    "    return stdout_text\n",
    "\n",
    "# Convert the score from 1 -5 to binary: 0 for dislike, 1 for like\n",
    "# \"Neutral\" score, for example -2 2, is cast as 0\n",
    "def score_to_binary(score_original):\n",
    "    binary_score = score_original.split(' ')\n",
    "    binary_score = list(binary_score)\n",
    "    binary_score = [1 if int(binary_score[i]) > abs(int(binary_score[i+1])) else 0 for i in range(0, len(binary_score), 3)]\n",
    "    return binary_score[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SentiStrength usage example\n",
    "\n",
    "example_sentence = \"'What a lovely day!'\"\n",
    "\n",
    "rated = rate_sentiment(example_sentence)\n",
    "converted = score_to_binary(rated)\n",
    "\n",
    "print(\"\\nRating the sentence:\")\n",
    "print(example_sentence)\n",
    "\n",
    "print(\"\\nRated by SentiStrength:\")\n",
    "print(rated)\n",
    "\n",
    "print(\"\\nRating converted to 0 for dislike / 1 for like:\")\n",
    "print(converted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Collect sentiments evaluated by SentiStrength for each review\n",
    "# ! Running this takes a few minutes (approximately 3-5min)\n",
    "\n",
    "sentistrength_sentiments = []\n",
    "\n",
    "for row in review_column:\n",
    "    rated_sentiment = rate_sentiment(row)\n",
    "    converted_sentiment = score_to_binary(rated_sentiment)\n",
    "    sentistrength_sentiments.append(converted_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Collect sentiments evaluated by SentiStrength for each review\n",
    "# ! Running this takes a few minutes (approximately 3-5min)\n",
    "\n",
    "# Progress printing function, elapsed_time should be in seconds\n",
    "def print_progress(completed_items, total_items, elapsed_time):\n",
    "    eta = (elapsed_time)/completed_items*(total_items - completed_items)\n",
    "    processed = completed_items/total_items*100.0\n",
    "    print(f\"Progress: {np.round(processed,1)}% ETA: {np.round(eta,1)}s   \", end=\"\\r\", flush=True)\n",
    "\n",
    "\n",
    "sentistrength_sentiments = []\n",
    "start = timer()\n",
    "progress = 0\n",
    "\n",
    "for row in review_column:\n",
    "    rated_sentiment = rate_sentiment(row)\n",
    "    converted_sentiment = score_to_binary(rated_sentiment)\n",
    "    sentistrength_sentiments.append(converted_sentiment)\n",
    "    progress += 1\n",
    "    print_progress(progress, len(review_column), timer() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate Pearson correlation between the SentimentStrength evaluations and the liked column\n",
    "\n",
    "pearson_corr_coeff = np.corrcoef(sentistrength_sentiments, liked_column)\n",
    "\n",
    "print(f\"\\nPearson correlation coefficient matrix:\\n{pearson_corr_coeff}\")\n",
    "print(f\"\\nPearson correlation between textblob sentiment and review liked score:\\n{pearson_corr_coeff[1][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 2\n",
    "\n",
    "Repeat this process when considering the correlation of the positive class alone and the correlation of the negative class alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate overall cosine similarity\n",
    "cosine_similarity = cosine(sentistrength_sentiments, liked_column)\n",
    "print(\"\\nOverall cosine similarity:\", cosine_similarity)\n",
    "\n",
    "\n",
    "# Calculate cosine similarity of positive class alone\n",
    "positive_sentistrength_sentiments = []\n",
    "positive_corresponding_liked_column = []\n",
    "\n",
    "for index, rating in enumerate(sentistrength_sentiments):\n",
    "    if rating == 1:\n",
    "        positive_sentistrength_sentiments.append(sentistrength_sentiments[index])\n",
    "        positive_corresponding_liked_column.append(liked_column[index])\n",
    "\n",
    "positive_cosine_similarity = cosine(positive_sentistrength_sentiments, positive_corresponding_liked_column)\n",
    "print(\"Positive cosine similarity:\", positive_cosine_similarity)\n",
    "\n",
    "\n",
    "# Calculate cosine similarity of negative class alone\n",
    "negative_sentistrength_sentiments = []\n",
    "negative_corresponding_liked_column = []\n",
    "\n",
    "for index, rating in enumerate(sentistrength_sentiments):\n",
    "    if rating == 0:\n",
    "        # Add miniscule amount to the 0 vector to avoid division by 0 in the cosine similarity calculation\n",
    "        negative_sentistrength_sentiments.append(sentistrength_sentiments[index] + 0.00000000000000001)\n",
    "        negative_corresponding_liked_column.append(liked_column[index])\n",
    "\n",
    "negative_cosine_similarity = cosine(negative_sentistrength_sentiments, negative_corresponding_liked_column)\n",
    "print(\"Negative cosine similarity:\", negative_cosine_similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 3\n",
    "\n",
    "Now we want to test the correlation with respect to some stylistic aspects of the review. Write a script that estimate\n",
    " the number of personal pronouns and number of adjectives and number of adverbs using part-of-speech tagger of your\n",
    "  choice. Compute both the cosine similarity between each of the above attributes (number of pronouns, number of\n",
    "   adjectives, number of adverbs) and the annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Printing utility functions\n",
    "def print_pearson_corr(first_thing_name, second_thing_name, pearson_corr_coeff):\n",
    "    print(f\"Pearson correlation between {first_thing_name} and {second_thing_name}:\\n{pearson_corr_coeff[1][0]}\\n\")\n",
    "\n",
    "def print_cosine_similarity(first_thing_name, second_thing_name, cosine_similarity):\n",
    "    print(f\"Cosine similarity between {first_thing_name} and {second_thing_name}:\\n{cosine_similarity}\\n\")\n",
    "\n",
    "\n",
    "pronouns_in_review = []\n",
    "adjectives_in_review = []\n",
    "adverbs_in_review = []\n",
    "\n",
    "# Count the pronouns, adjectives and adverbs in each review and add them to the corresponding lists\n",
    "for row in review_column:\n",
    "    pronoun_count = 0\n",
    "    adjective_count = 0\n",
    "    adverb_count = 0\n",
    "\n",
    "    for word in row.split(\" \"):\n",
    "        # Part of speech tag, pos_tag reference: https://www.guru99.com/pos-tagging-chunking-nltk.html\n",
    "        word_pos = pos_tag(word_tokenize(word))\n",
    "\n",
    "        if word_pos[0][1] == \"PRP\":\n",
    "            pronoun_count += 1\n",
    "        if word_pos[0][1] == \"JJ\" or word_pos[0][1] == \"JJR\" or word_pos[0][1] == \"JJS\":\n",
    "            adjective_count += 1\n",
    "        if word_pos[0][1] == \"RB\" or word_pos[0][1] == \"WBR\":\n",
    "            adverb_count += 1\n",
    "\n",
    "    pronouns_in_review.append(pronoun_count)\n",
    "    adjectives_in_review.append(adjective_count)\n",
    "    adverbs_in_review.append(adverb_count)\n",
    "\n",
    "\n",
    "print(\"\\n---PEARSON CORRELATIONS---\\n\")\n",
    "pearson_corr_coeff_pronoun = np.corrcoef(pronouns_in_review, liked_column)\n",
    "pearson_corr_coeff_adjective = np.corrcoef(adjectives_in_review, liked_column)\n",
    "pearson_corr_coeff_adverb = np.corrcoef(adverbs_in_review, liked_column)\n",
    "print_pearson_corr(\"number of PRONOUNS\", \"review liked score\", pearson_corr_coeff_pronoun)\n",
    "print_pearson_corr(\"number of ADJECTIVES\", \"review liked score\", pearson_corr_coeff_adjective)\n",
    "print_pearson_corr(\"number of ADVERBS\", \"review liked score\", pearson_corr_coeff_adverb)\n",
    "\n",
    "print(\"\\n---COSINE SIMILARITIES---\\n\")\n",
    "cosine_similarity_pronoun = cosine(pronouns_in_review, liked_column)\n",
    "cosine_similarity_adjective = cosine(adjectives_in_review, liked_column)\n",
    "cosine_similarity_adverb = cosine(adverbs_in_review, liked_column)\n",
    "print_cosine_similarity(\"number of PRONOUNS\", \"review liked score\", cosine_similarity_pronoun)\n",
    "print_cosine_similarity(\"number of ADJECTIVES\", \"review liked score\", cosine_similarity_adjective)\n",
    "print_cosine_similarity(\"number of ADVERBS\", \"review liked score\", cosine_similarity_adverb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 4\n",
    "\n",
    "We want to test the hypothesis that the opinion of about the restaurant is constructed according to Price, Quality\n",
    " of food served in the restaurant, and friendly staff. Suggest a script that allows you to identify Review that are\n",
    "  more focused on price, quality of food, friendly staff. You may consider a set of keywords that are most suitable\n",
    "   to each category and then use simple string matching to match this effect. For each category, generate a binary\n",
    "    vector indicating whether the given review focuses on the corresponding category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def any_keywords_in_string(keywords, row):\n",
    "    words = word_tokenize(row.lower())\n",
    "    words = [WordNetLemmatizer().lemmatize(word) for word in words] # Lemmatisation\n",
    "\n",
    "    if any(keyword in words for keyword in keywords):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "\n",
    "with open(\"keywords.json\", \"r\") as keyword_file:\n",
    "    keywords = json.loads(keyword_file.read())\n",
    "\n",
    "price_focus = []\n",
    "quality_focus = []\n",
    "staff_focus = []\n",
    "\n",
    "for row in review_column:\n",
    "    price_focus.append(any_keywords_in_string(keywords[\"price\"], row))\n",
    "    quality_focus.append(any_keywords_in_string(keywords[\"quality\"], row))\n",
    "    staff_focus.append(any_keywords_in_string(keywords[\"staff\"], row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 5\n",
    "\n",
    "Estimate the correlation using Pearson correlation between each vector category and the data annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pearson_corr_coeff_price = np.corrcoef(price_focus, liked_column)\n",
    "pearson_corr_coeff_quality = np.corrcoef(quality_focus, liked_column)\n",
    "pearson_corr_coeff_staff = np.corrcoef(staff_focus, liked_column)\n",
    "\n",
    "print_pearson_corr(\"PRICE\",\"liked score\", pearson_corr_coeff_price)\n",
    "print_pearson_corr(\"QUALITY\",\"liked score\", pearson_corr_coeff_quality)\n",
    "print_pearson_corr(\"STAFF\", \"liked score\", pearson_corr_coeff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 6\n",
    "\n",
    "We want to revisit the construction of the categories in 4). Instead of string matching, use the semantic\n",
    "similarity in the following way. Calculate the Wu and Palmer similarity between âpriceâ and the Review (using\n",
    "the sentence-to-sentence similarity as in labs), repeat this process for the other three categories by suggestion\n",
    "a representative keyword (s) that will be used to calculate sentence-to-sentence similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Either T1:[] or T2:['wow', 'loved', 'place'] is empty after preProcessing\n",
      "Either T1:[] or T2:['wow', 'loved', 'place'] is empty after preProcessing\n",
      "Either T1:[] or T2:['crust', 'good'] is empty after preProcessing\n",
      "Either T1:[] or T2:['crust', 'good'] is empty after preProcessing\n",
      "Either T1:[] or T2:['tasty', 'texture', 'wa', 'nasty'] is empty after preProcessing\n",
      "Either T1:[] or T2:['tasty', 'texture', 'wa', 'nasty'] is empty after preProcessing\n",
      "Either T1:[] or T2:['stopped', 'late', 'may', 'bank', 'holiday', 'rick', 'steve', 'recommendation', 'loved'] is empty after preProcessing\n",
      "Either T1:[] or T2:['stopped', 'late', 'may', 'bank', 'holiday', 'rick', 'steve', 'recommendation', 'loved'] is empty after preProcessing\n",
      "Either T1:[] or T2:['selection', 'menu', 'wa', 'great', 'price'] is empty after preProcessing\n",
      "Either T1:[] or T2:['selection', 'menu', 'wa', 'great', 'price'] is empty after preProcessing\n",
      "Either T1:[] or T2:['getting', 'angry', 'want', 'damn', 'pho'] is empty after preProcessing\n",
      "Either T1:[] or T2:['getting', 'angry', 'want', 'damn', 'pho'] is empty after preProcessing\n",
      "Either T1:[] or T2:['honeslty', 'taste', 'fresh'] is empty after preProcessing\n",
      "Either T1:[] or T2:['honeslty', 'taste', 'fresh'] is empty after preProcessing\n",
      "Either T1:[] or T2:['potato', 'like', 'rubber', 'could', 'tell', 'made', 'ahead', 'time', 'kept', 'warmer'] is empty after preProcessing\n",
      "Either T1:[] or T2:['potato', 'like', 'rubber', 'could', 'tell', 'made', 'ahead', 'time', 'kept', 'warmer'] is empty after preProcessing\n",
      "Either T1:[] or T2:['fry', 'great'] is empty after preProcessing\n",
      "Either T1:[] or T2:['fry', 'great'] is empty after preProcessing\n",
      "Either T1:[] or T2:['great', 'touch'] is empty after preProcessing\n",
      "Either T1:[] or T2:['great', 'touch'] is empty after preProcessing\n",
      "Either T1:[] or T2:['service', 'wa', 'prompt'] is empty after preProcessing\n",
      "Either T1:[] or T2:['service', 'wa', 'prompt'] is empty after preProcessing\n",
      "Either T1:[] or T2:['would', 'go', 'back'] is empty after preProcessing\n",
      "Either T1:[] or T2:['would', 'go', 'back'] is empty after preProcessing\n",
      "Either T1:[] or T2:['cashier', 'care', 'ever', 'say', 'still', 'ended', 'wayyy', 'overpriced'] is empty after preProcessing\n",
      "Either T1:[] or T2:['cashier', 'care', 'ever', 'say', 'still', 'ended', 'wayyy', 'overpriced'] is empty after preProcessing\n",
      "Either T1:[] or T2:['tried', 'cape', 'cod', 'ravoli', 'chicken', 'cranberry', 'mmmm'] is empty after preProcessing\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 124\u001b[0m\n\u001b[1;32m    122\u001b[0m     price_focus_wup\u001b[39m.\u001b[39mappend(best_wup_similarity(keywords[\u001b[39m\"\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m\"\u001b[39m], row))\n\u001b[1;32m    123\u001b[0m     quality_focus_wup\u001b[39m.\u001b[39mappend(best_wup_similarity(keywords[\u001b[39m\"\u001b[39m\u001b[39mquality\u001b[39m\u001b[39m\"\u001b[39m], row))\n\u001b[0;32m--> 124\u001b[0m     staff_focus_wup\u001b[39m.\u001b[39mappend(best_wup_similarity(keywords[\u001b[39m\"\u001b[39;49m\u001b[39mstaff\u001b[39;49m\u001b[39m\"\u001b[39;49m], row))\n\u001b[1;32m    125\u001b[0m     print_progress(progress, \u001b[39mlen\u001b[39m(review_column),timer() \u001b[39m-\u001b[39m start)\n\u001b[1;32m    128\u001b[0m pearson_corr_coeff_price_wup \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcorrcoef(price_focus_wup, liked_column)\n",
      "Cell \u001b[0;32mIn [23], line 106\u001b[0m, in \u001b[0;36mbest_wup_similarity\u001b[0;34m(keywords, row)\u001b[0m\n\u001b[1;32m    104\u001b[0m     string\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mi\n\u001b[1;32m    105\u001b[0m     string\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 106\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39;49m(similarity(keyword, string) \u001b[39mfor\u001b[39;49;00m keyword \u001b[39min\u001b[39;49;00m keywords)\n",
      "Cell \u001b[0;32mIn [23], line 106\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    104\u001b[0m     string\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mi\n\u001b[1;32m    105\u001b[0m     string\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 106\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(similarity(keyword, string) \u001b[39mfor\u001b[39;00m keyword \u001b[39min\u001b[39;00m keywords)\n",
      "Cell \u001b[0;32mIn [23], line 79\u001b[0m, in \u001b[0;36msimilarity\u001b[0;34m(T1, T2)\u001b[0m\n\u001b[1;32m     77\u001b[0m Max \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     78\u001b[0m \u001b[39mfor\u001b[39;00m w1 \u001b[39min\u001b[39;00m words1:\n\u001b[0;32m---> 79\u001b[0m     score \u001b[39m=\u001b[39m word_similarity(w1,w2)\n\u001b[1;32m     80\u001b[0m     \u001b[39mif\u001b[39;00m Max \u001b[39m<\u001b[39m score:\n\u001b[1;32m     81\u001b[0m        Max \u001b[39m=\u001b[39m score\n",
      "Cell \u001b[0;32mIn [23], line 32\u001b[0m, in \u001b[0;36mword_similarity\u001b[0;34m(w1, w2)\u001b[0m\n\u001b[1;32m     30\u001b[0m S2 \u001b[39m=\u001b[39m wn\u001b[39m.\u001b[39msynsets(w2)\n\u001b[1;32m     31\u001b[0m \u001b[39mif\u001b[39;00m S1 \u001b[39mand\u001b[39;00m S2:\n\u001b[0;32m---> 32\u001b[0m    similarity \u001b[39m=\u001b[39m wup(S1[\u001b[39m0\u001b[39;49m], S2[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     33\u001b[0m    \u001b[39mif\u001b[39;00m similarity:\n\u001b[1;32m     34\u001b[0m       \u001b[39mreturn\u001b[39;00m \u001b[39mround\u001b[39m(similarity,\u001b[39m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn [23], line 14\u001b[0m, in \u001b[0;36mwup\u001b[0;34m(S1, S2)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwup\u001b[39m(S1, S2):\n\u001b[0;32m---> 14\u001b[0m     \u001b[39mreturn\u001b[39;00m S1\u001b[39m.\u001b[39;49mwup_similarity(S2)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/corpus/reader/wordnet.py:955\u001b[0m, in \u001b[0;36mSynset.wup_similarity\u001b[0;34m(self, other, verbose, simulate_root)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwup_similarity\u001b[39m(\u001b[39mself\u001b[39m, other, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, simulate_root\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    921\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    922\u001b[0m \u001b[39m    Wu-Palmer Similarity:\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[39m    Return a score denoting how similar two word senses are, based on the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    953\u001b[0m \n\u001b[1;32m    954\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 955\u001b[0m     need_root \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_needs_root() \u001b[39mor\u001b[39;00m other\u001b[39m.\u001b[39m_needs_root()\n\u001b[1;32m    957\u001b[0m     \u001b[39m# Note that to preserve behavior from NLTK2 we set use_min_depth=True\u001b[39;00m\n\u001b[1;32m    958\u001b[0m     \u001b[39m# It is possible that more accurate results could be obtained by\u001b[39;00m\n\u001b[1;32m    959\u001b[0m     \u001b[39m# removing this setting and it should be tested later on\u001b[39;00m\n\u001b[1;32m    960\u001b[0m     subsumers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlowest_common_hypernyms(\n\u001b[1;32m    961\u001b[0m         other, simulate_root\u001b[39m=\u001b[39msimulate_root \u001b[39mand\u001b[39;00m need_root, use_min_depth\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    962\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/corpus/reader/wordnet.py:474\u001b[0m, in \u001b[0;36mSynset._needs_root\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_needs_root\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 474\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pos \u001b[39m==\u001b[39m NOUN \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wordnet_corpus_reader\u001b[39m.\u001b[39;49mget_version() \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1.6\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    475\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/corpus/reader/wordnet.py:1380\u001b[0m, in \u001b[0;36mWordNetCorpusReader.get_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1378\u001b[0m fh \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_file(ADJ)\n\u001b[1;32m   1379\u001b[0m fh\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n\u001b[0;32m-> 1380\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m fh:\n\u001b[1;32m   1381\u001b[0m     match \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msearch(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWord[nN]et (\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+|\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+) Copyright\u001b[39m\u001b[39m\"\u001b[39m, line)\n\u001b[1;32m   1382\u001b[0m     \u001b[39mif\u001b[39;00m match \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/data.py:1151\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[0;32m-> 1151\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1152\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext()\n\u001b[1;32m   1154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Task 6 TODO\n",
    "\n",
    "\"\"\"\n",
    "Functions wup, preProcess, word_similarity and similarity originally by:\n",
    "Created on Wed Sep 23 07:17:05 2020\n",
    "@author: Yazid BOUNAB\n",
    "Yazid.Bounab@oulu.fi\n",
    "\n",
    "\"\"\"\n",
    "Stopwords = list(set(nltk.corpus.stopwords.words('english')))\n",
    "stemmer = SnowballStemmer(\"english\")#we will avoid the stemming because it will give a pbm with sysnset search\n",
    "\n",
    "def wup(S1, S2):\n",
    "    return S1.wup_similarity(S2)\n",
    "\n",
    "\n",
    "def preProcess(sentence):\n",
    "    #Stopwords = list(set(nltk.corpus.stopwords.words('english')))\n",
    "    #stemmer = SnowballStemmer(\"english\")#we will avoid the stemming because it will give a pbm with sysnset search\n",
    "    words = word_tokenize(sentence)\n",
    "    #words = [stemmer.stem(word) for word in words] \n",
    "    words = [word.lower() for word in words if word.isalpha() and word not in Stopwords] #get rid of numbers and Stopwords\n",
    "    return words\n",
    "\n",
    "\n",
    "\n",
    "def word_similarity(w1,w2):\n",
    "\n",
    "    S1 = wn.synsets(w1)\n",
    "    S2 = wn.synsets(w2)\n",
    "    if S1 and S2:\n",
    "       similarity = wup(S1[0], S2[0])\n",
    "       if similarity:\n",
    "          return round(similarity,2)\n",
    "    return 0\n",
    "\n",
    "def similarity(T1, T2):\n",
    "    if(len(T1) > 1):\n",
    "        words1 = preProcess(T1)\n",
    "    else:\n",
    "        words1 = word_tokenize(T1)\n",
    "    if(len(T2) > 1):\n",
    "        words2 = preProcess(T2)\n",
    "    else:\n",
    "        words2 = word_tokenize(T2)\n",
    "    tf = TfidfVectorizer(use_idf=True)\n",
    "    if(words1 and words2):\n",
    "        tf.fit_transform([' '.join(words1), ' '.join(words2)])\n",
    "    else:\n",
    "        print(f\"Either T1:{words1} or T2:{words2} is empty after preProcessing\")\n",
    "        return 0\n",
    "    Idf = dict(zip(tf.get_feature_names_out(), tf.idf_))\n",
    "    Sim = 0\n",
    "    Sim_score1 = 0\n",
    "    Sim_score2 = 0\n",
    "\n",
    "    #print(f\"{T1}\\n{T2}\\n---------\")\n",
    "    for w1 in words1:\n",
    "        Max = 0\n",
    "        for w2 in words2:\n",
    "            score = word_similarity(w1,w2)\n",
    "            if Max < score:\n",
    "               Max = score\n",
    "        try:\n",
    "            Sim_score1 += Max*Idf[w1]\n",
    "        except KeyError:\n",
    "            print(f\"{w1} not in Idf\")\n",
    "\n",
    "    try:\n",
    "        Sim_score1 /= sum([Idf[w1] for w1 in words1])\n",
    "    except KeyError:\n",
    "        print(f\"{w1} not in Idf\")\n",
    "    except ZeroDivisionError:\n",
    "        Sim_score1 = 0\n",
    "\n",
    "    for w2 in words2:\n",
    "        Max = 0\n",
    "        for w1 in words1:\n",
    "            score = word_similarity(w1,w2)\n",
    "            if Max < score:\n",
    "               Max = score\n",
    "        try:\n",
    "            Sim_score2 += Max*Idf[w2]\n",
    "        except KeyError:\n",
    "            print(f\"{w2} not in Idf\")\n",
    "\n",
    "    try:\n",
    "        Sim_score2 /= sum([Idf[w2] for w2 in words2])\n",
    "    except KeyError:\n",
    "        print(f\"{w2} not in Idf\")\n",
    "    except ZeroDivisionError:\n",
    "        Sim_score2 = 0\n",
    "\n",
    "    Sim = (Sim_score1+Sim_score2)/2\n",
    "    #print(round(Sim,2))\n",
    "    return round(Sim,2)\n",
    "\n",
    "\n",
    "def best_wup_similarity(keywords, row):\n",
    "    words = word_tokenize(row.lower())\n",
    "    words = [WordNetLemmatizer().lemmatize(word) for word in words] # Lemmatisation\n",
    "    string = \"\"\n",
    "    for i in words:\n",
    "        string+=i\n",
    "        string+=\" \"\n",
    "    return max(similarity(keyword, string) for keyword in keywords)\n",
    "\n",
    "with open(\"keywords.json\", \"r\") as keyword_file:\n",
    "    keywords = json.loads(keyword_file.read())\n",
    "\n",
    "\n",
    "\"\"\" Beware, this commented part might run for 30 minutes!\n",
    "\n",
    "price_focus_wup = []\n",
    "quality_focus_wup = []\n",
    "staff_focus_wup = []\n",
    "\n",
    "progress = 0\n",
    "start = timer()\n",
    "for row in review_column:\n",
    "    progress += 1\n",
    "    price_focus_wup.append(best_wup_similarity(keywords[\"price\"], row))\n",
    "    quality_focus_wup.append(best_wup_similarity(keywords[\"quality\"], row))\n",
    "    staff_focus_wup.append(best_wup_similarity(keywords[\"staff\"], row))\n",
    "    print_progress(progress, len(review_column),timer() - start)\n",
    "\"\"\"\n",
    "\n",
    "pearson_corr_coeff_price_wup = np.corrcoef(price_focus_wup, liked_column)\n",
    "pearson_corr_coeff_quality_wup = np.corrcoef(quality_focus_wup, liked_column)\n",
    "pearson_corr_coeff_staff_wup = np.corrcoef(staff_focus_wup, liked_column)\n",
    "\n",
    "print_pearson_corr(\"PRICE\",\"liked score\", pearson_corr_coeff_price_wup)\n",
    "print_pearson_corr(\"QUALITY\",\"liked score\", pearson_corr_coeff_quality_wup)\n",
    "print_pearson_corr(\"STAFF\", \"liked score\", pearson_corr_coeff_staff_wup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 7\n",
    "\n",
    "We want to test another approach for computing the categories by using the empath categories embedding. For this\n",
    "purpose, re-visit the naming of the empath-categories in GitHub - Ejhfast/empath-client: analyze text with empath\n",
    "(https://github.com/Ejhfast/empath-client) and select those that might be linked to Price, Quality, Staff friendship.\n",
    "Write a code that allows you to determine appropriate categories from this embedding and then calculate the correlation\n",
    "score.  Alternative to manual scrutinization of the Empath categories, you may also generate an empath category\n",
    "embedding for the keyword âpriceâ, âfood qualityâ, âfriendly staffâ, and then compute cosine similarity between the\n",
    "Review embedding vector and each of the above four embedding vectors, so that the one that yields the highest similarity\n",
    "score will be considered as the one that best represents the underlined category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lexicon = Empath()\n",
    "\n",
    "print(\"\\nPrice category:\")\n",
    "lexicon.create_category(\"price\", keywords[\"price\"], size=50)\n",
    "\n",
    "print(\"\\nFood quality category:\")\n",
    "lexicon.create_category(\"quality\", keywords[\"quality\"], size=50)\n",
    "\n",
    "print(\"\\nStaff friendliness category:\")\n",
    "lexicon.create_category(\"staff\", keywords[\"staff\"], size=50)\n",
    "\n",
    "empath_price = []\n",
    "empath_quality = []\n",
    "empath_staff = []\n",
    "\n",
    "for row in review_column:\n",
    "    empath_price.append(lexicon.analyze(row, categories=[\"price\"], normalize=True).get(\"price\"))\n",
    "    empath_quality.append(lexicon.analyze(row, categories=[\"quality\"], normalize=True).get(\"quality\"))\n",
    "    empath_staff.append(lexicon.analyze(row, categories=[\"staff\"], normalize=True).get(\"staff\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n---EMPATH COSINE SIMILARITIES---\\n\")\n",
    "\n",
    "cosine_sim_empath_price = cosine(empath_price, liked_column)\n",
    "cosine_sim_empath_quality = cosine(empath_quality, liked_column)\n",
    "cosine_sim_empath_staff = cosine(empath_staff, liked_column)\n",
    "print_cosine_similarity(\"number of Empath PRICE\", \"review liked score\", cosine_sim_empath_price)\n",
    "print_cosine_similarity(\"number of Empath QUALITY\", \"review liked score\", cosine_sim_empath_quality)\n",
    "print_cosine_similarity(\"number of Empath STAFF\", \"review liked score\", cosine_sim_empath_staff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 8\n",
    "\n",
    "We want to further emphasize on misclassified reviews. For this purpose, concatenate all reviews for which the\n",
    "sentiment score is positive while the annotation is zero and those for which the sentiment is zero while the\n",
    "annotation is 1. Construct the Wordcloud of this dataset. Write a histogram showing the 10 most common wordings\n",
    "in this dataset. Comment on the findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Collect misclassified ratings\n",
    "# ! Running this takes a few minutes (approximately 3-5min)\n",
    "\n",
    "misclassified_reviews = []\n",
    "\n",
    "for index, row in enumerate(review_column):\n",
    "    rated_sentiment = rate_sentiment(row)\n",
    "    converted_sentiment = score_to_binary(rated_sentiment)\n",
    "\n",
    "    if converted_sentiment != liked_column[index]:\n",
    "        misclassified_reviews.append([row, liked_column[index]])\n",
    "\n",
    "print(\"\\nNumber of misclassified ratings:\")\n",
    "print(len(misclassified_reviews))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pre-processing: lemmatisation, turn all chars to lowercase, leave out stopwords and words containing numbers\n",
    "# Returns a list of tokens\n",
    "def pre_process(doc_string):\n",
    "    # Use stopwords from nltk\n",
    "    STOPWORDS = list(set(nltk.corpus.stopwords.words('english')))\n",
    "\n",
    "    # Tokenize words and turn them to lowercase\n",
    "    words = word_tokenize(doc_string.lower())\n",
    "\n",
    "    # Lemmatisation\n",
    "    words = [WordNetLemmatizer().lemmatize(word, pos=\"v\") for word in words]\n",
    "\n",
    "    # Leave out stopwords and words containing numbers\n",
    "    words = [\n",
    "        word for word in words\n",
    "        if word.isalpha()\n",
    "        and word not in STOPWORDS\n",
    "    ]\n",
    "    return words\n",
    "\n",
    "\n",
    "# Pre-process the collected misclassified ratings\n",
    "misclassified_reviews_string = \"\"\n",
    "for review in misclassified_reviews:\n",
    "    misclassified_reviews_string += f\"{review[0]} \"\n",
    "misclassified_reviews_preprocessed = pre_process(misclassified_reviews_string)\n",
    "\n",
    "print(\"\\nNumber of word tokens after preprocessing:\")\n",
    "print(len(misclassified_reviews_preprocessed), \"\\n\")\n",
    "\n",
    "print(\"Word tokens after preprocessing (print truncated to first 50 elements):\")\n",
    "print(misclassified_reviews_preprocessed[:50], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Collect 10 most common words\n",
    "most_common_words = Counter(misclassified_reviews_preprocessed).most_common(10)\n",
    "\n",
    "# Parse the generated tuples into two different arrays\n",
    "words = [word for word, _ in most_common_words]\n",
    "counts = [counts for _, counts in most_common_words]\n",
    "\n",
    "# Plot the words and their frequency\n",
    "plt.bar(words, counts)\n",
    "plt.title(\"Histogram of the 10 most common words in misclassified reviews\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate word cloud of the misclassified reviews\n",
    "\n",
    "word_cloud = WordCloud(\n",
    "    width=3000,\n",
    "    height=2000,\n",
    "    random_state=1,\n",
    "    background_color=\"#1f1f36\",\n",
    "    colormap=\"Blues\",\n",
    "    collocations=False,\n",
    ").generate(\" \".join(misclassified_reviews_preprocessed))\n",
    "\n",
    "print(\"\\nWord cloud of the misclassified reviews:\")\n",
    "\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(word_cloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 9.\n",
    "\n",
    "Now we would like to build a machine learning model for sentiment analysis that takes into account the ambiguous\n",
    "cases identified in 9). For this purpose, write and script and review the preprocessing and stopword list to not\n",
    "discard relevant information in the context of sentiment analysis (e.g., avoid discarding negation cues, adjectives\n",
    "that subsumes polarity and apostrophes, lower-case as capitalization brings emotion,..), then use TfIdfVectorizer\n",
    "with a maximum feature set of 500, minimum 2 repetition and no more than 60% of word repetition across sentences.\n",
    "Build this model for one dataset using randomly selected 70% training and 30% testing. Report the classification\n",
    "accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_review_column = [review[0] for review in misclassified_reviews]\n",
    "misclassified_liked_column = [review[1] for review in misclassified_reviews]\n",
    "\n",
    "# Lemmatisation is the only type of pre-processing used for the machine learning models.\n",
    "# Removing stopwords discards important negation cues. For example, the word \"not\" -> \"not good\" becomes \"good\".\n",
    "# Turning the words to lowercase is not used because capitalization brings emotion. For example, FULL CAPITALIZATION.\n",
    "misclassified_review_column = [WordNetLemmatizer().lemmatize(word) for word in misclassified_review_column]\n",
    "\n",
    "# Divide the reviews into training and test data\n",
    "review_train, review_test, liked_train, liked_test = train_test_split(\n",
    "    misclassified_review_column,\n",
    "    misclassified_liked_column,\n",
    "    test_size=0.3\n",
    ")\n",
    "\n",
    "# Use tf-idf vectorizer to fit and transform review training data\n",
    "TFIDF = TfidfVectorizer(max_features=500, max_df=0.60, min_df=2) \n",
    "tfidf_fit_trans_review = TFIDF.fit_transform(review_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Support vector classification model\n",
    "svc_model = SVC()\n",
    "\n",
    "# Train the model with the review column tf-idf fitted and transformed training data, and liked column training data\n",
    "svc_model.fit(\n",
    "    tfidf_fit_trans_review,\n",
    "    liked_train\n",
    ")\n",
    "\n",
    "# Make predictions using the model\n",
    "svc_model_predictions = svc_model.predict(\n",
    "    TFIDF.transform(review_test)\n",
    ")\n",
    "\n",
    "# Get accuracy of the predictions\n",
    "svc_model_accuracy = accuracy_score(liked_test, svc_model_predictions)\n",
    "\n",
    "print(f\"\\nSupport vector classification model accuracy:\\n{svc_model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Logistic regression model\n",
    "log_reg_model = LogisticRegression()\n",
    "\n",
    "# Train the model with the review column tf-idf fitted and transformed training data, and liked column training data\n",
    "log_reg_model.fit(\n",
    "    tfidf_fit_trans_review,\n",
    "    liked_train\n",
    ")\n",
    "\n",
    "# Make predictions using the model\n",
    "log_reg_model_predictions = log_reg_model.predict(\n",
    "    TFIDF.transform(review_test)\n",
    ")\n",
    "\n",
    "# Get accuracy of the predictions\n",
    "log_reg_model_accuracy = accuracy_score(liked_test, log_reg_model_predictions)\n",
    "\n",
    "print(f\"\\nSupport vector classification model accuracy:\\n{log_reg_model_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 10\n",
    "\n",
    "Use Glove embedding instead of TfidfVectorizer, see GloVe: Global Vectors for Word Representation\n",
    "(https://nlp.stanford.edu/projects/glove/). Use the Glove embedding as feature vectors and test the performance in\n",
    "the original data (30% test data) and report the classification accuracy on the other two datasets. Comment on the\n",
    "limitations of the approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Task 11\n",
    "\n",
    "Identify appropriate literature to comment on your findings and methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Task 11 TODO\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c5a4743cda23ca51c4316defd6eef32baef9dc6029e48c24d82dfe2c9f06bc0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
